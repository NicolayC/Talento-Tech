{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUn2+ZqtoWOea9oMupVGpe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3uVsqu2zCBps"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Implementación de Q-Learning en un entorno de Gridworld simple"],"metadata":{"id":"k9z-k0o7CJF4"}},{"cell_type":"code","source":["import numpy as np\n","\n","gridworld = np.array([\n","    [-1, -1, -1, 1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1]\n","])\n","\n","acciones = [(0, -1), (0, 1), (-1, 0), (1, 0),]\n","\n","Q = np.zeros((gridworld.shape[0], gridworld.shape[1], len(acciones)))\n","\n","gamma = 0.8\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado = (0, 0)\n","    while estado != (0, 3):\n","        accion = np.random.choice(range(len(acciones)))\n","        nueva_fila = estado[0] + acciones[accion][0]\n","        nueva_columna = estado[1] + acciones[accion][1]\n","        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_columna < gridworld.shape[1]:\n","          recompensa = gridworld[nueva_fila, nueva_columna]\n","          nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","          Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n","          estado = (nueva_fila, nueva_columna)\n","\n","print(\"Matriz Q final:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCOSnmICCXwn","executionInfo":{"status":"ok","timestamp":1725890364304,"user_tz":300,"elapsed":1887,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"c05615f5-88cf-4250-ec59-d2c95d49d1cc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz Q final:\n","[[[ 0.   -1.    0.   -1.  ]\n","  [-1.   -0.2   0.   -1.8 ]\n","  [-1.    1.    0.   -1.16]\n","  [ 0.    0.    0.    0.  ]]\n","\n"," [[ 0.   -1.8  -1.   -1.  ]\n","  [-1.   -1.16 -1.   -1.8 ]\n","  [-1.8  -0.2  -0.2  -1.8 ]\n","  [-1.16  0.    1.   -1.  ]]\n","\n"," [[ 0.   -1.8  -1.   -1.  ]\n","  [-1.   -1.8  -1.8  -1.  ]\n","  [-1.8  -1.   -1.16 -1.  ]\n","  [-1.8   0.   -0.2  -1.  ]]\n","\n"," [[ 0.   -1.   -1.    0.  ]\n","  [-1.   -1.   -1.8   0.  ]\n","  [-1.   -1.   -1.8   0.  ]\n","  [-1.    0.   -1.    0.  ]]]\n"]}]},{"cell_type":"markdown","source":["Aplicación del Aprendizaje por Refuerzo en Juegos"],"metadata":{"id":"qIXyDEAgFV-g"}},{"cell_type":"code","source":["import numpy as np\n","\n","recompensas = {\n","    'ganar': 1,\n","    'perder': -1,\n","    'empatar': 0\n","}\n","Q = {}\n","\n","def q_learning_juego(estado_actual, accion, nuevo_estado, resultado):\n","    if estado_actual not in Q:\n","      Q[estado_actual] = np.zeros(len(acciones))\n","    if nuevo_estado not in Q:\n","      Q[nuevo_estado] = np.zeros(len(acciones))\n","      nuevo_valor = recompensas[resultado] + gamma * np.max(Q[nuevo_estado])\n","      Q[estado_actual][accion] = (1 - alpha) * Q[estado_actual][accion] + alpha * nuevo_valor\n","      return Q[estado_actual][accion]\n","\n","print(\"Matriz Q final:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Xt55QxeFSii","executionInfo":{"status":"ok","timestamp":1725890790929,"user_tz":300,"elapsed":215,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"da7de3fe-4c66-4972-fbef-59ddfa545ff7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz Q final:\n","{}\n"]}]},{"cell_type":"markdown","source":["Aplicación del Aprendizaje por Refuerzo en robótica"],"metadata":{"id":"Eg5kxQ3GG_kA"}},{"cell_type":"code","source":["import numpy as np\n","\n","entorno = np.array([\n","    [0, 0, 0, 0, 0],\n","    [0, -1, -1, -1, 0],\n","    [0, 0, -1, 0, 0],\n","    [0, -1, -1, -1, 0],\n","    [0, 0, 0, 0, 0]\n","])\n","\n","acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n","\n","Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado = (0, 0)\n","    max_steps = 100\n","    paso = 0\n","    while paso < max_steps:\n","        accion = np.random.choice(range(len(acciones)))\n","        nueva_fila = estado[0] + acciones[accion][0]\n","        nueva_columna = estado[1] + acciones[accion][1]\n","        if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_columna < entorno.shape[1]:\n","            recompensa = entorno[nueva_fila, nueva_columna]\n","            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n","            estado = (nueva_fila, nueva_columna)\n","        paso += 1\n","\n","print(\"Matriz Q final:\")\n","print(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXoY0AXGHJ_2","executionInfo":{"status":"ok","timestamp":1725891852956,"user_tz":300,"elapsed":5187,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"dd667672-8045-4765-f1d9-1b951e61fb89"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz Q final:\n","[[[ 0.  0.  0.  0.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0.  0.]]\n","\n"," [[ 0. -1.  0.  0.]\n","  [ 0. -1.  0.  0.]\n","  [-1. -1.  0. -1.]\n","  [-1.  0.  0.  0.]\n","  [-1.  0.  0.  0.]]\n","\n"," [[ 0.  0.  0.  0.]\n","  [ 0. -1. -1. -1.]\n","  [ 0.  0. -1. -1.]\n","  [-1.  0. -1. -1.]\n","  [ 0.  0.  0.  0.]]\n","\n"," [[ 0. -1.  0.  0.]\n","  [ 0. -1.  0.  0.]\n","  [-1. -1. -1.  0.]\n","  [-1.  0.  0.  0.]\n","  [-1.  0.  0.  0.]]\n","\n"," [[ 0.  0.  0.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0.  0.  0.]]]\n"]}]},{"cell_type":"markdown","source":["Aplicación del Aprendizaje por Refuerzo en gestión de recursos"],"metadata":{"id":"0pOEW9eiKl3A"}},{"cell_type":"code","source":["import numpy as np\n","\n","estados = ['Bajo', 'Medio', 'Alto']\n","acciones = ['Reabastecer', 'No reabastecer']\n","\n","recompensas = {\n","    ('Bajo', 'Reabastecer'): 50,\n","    ('Bajo', 'No reabastecer'): -10,\n","    ('Medio', 'Reabastecer'): 30,\n","    ('Medio', 'No reabastecer'): 0,\n","    ('Alto', 'Reabastecer'): 10,\n","    ('Alto', 'No reabastecer'): -20,\n","}\n","\n","Q = {}\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado_actual = np.random.choice(estados)\n","    while True:\n","        accion = np.random.choice(acciones)\n","        recompensa = recompensas[(estado_actual, accion)]\n","        if estado_actual not in Q:\n","            Q[estado_actual] = {}\n","        if accion not in Q[estado_actual]:\n","            Q[estado_actual][accion] = 0\n","        nuevo_estado = np.random.choice(estados)\n","        max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n","        Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n","        estado_actual = nuevo_estado\n","        if recompensa == 50 or recompensa == 30 or recompensa == 10:\n","            break\n","\n","print(\"Matriz Q final:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZvoXn_FKuM7","executionInfo":{"status":"ok","timestamp":1725892406719,"user_tz":300,"elapsed":227,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"abc86026-f389-4f0b-8df9-1b34b45e5e61"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz Q final:\n","{'Bajo': {'No reabastecer': 240.30587031889615, 'Reabastecer': 308.2547185360419}, 'Alto': {'Reabastecer': 250.45480628389726, 'No reabastecer': 227.51887057992008}, 'Medio': {'Reabastecer': 283.08167911093983, 'No reabastecer': 250.9276912460795}}\n"]}]}]}