{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7U7iMglVsAA5O0RUM2x44"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Generación de un entorno de juego simple"],"metadata":{"id":"xHSJve6I7XWK"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FT8jii5n7GFH","executionInfo":{"status":"ok","timestamp":1725887960086,"user_tz":300,"elapsed":221,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"a39649f4-1490-47eb-836e-9d1f332869b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estados:  [0, 1, 2, 3]\n","Acciones:  [0, 1]\n","Recompensas:  {0: -1, 1: -1, 2: -1, 3: 10}\n"]}],"source":["class Environment:\n","    def __init__(self):\n","      self.state_space = [0, 1, 2, 3]\n","      self.action_space = [0, 1]\n","      self.rewards = {0: -1, 1: -1, 2: -1, 3: 10}\n","\n","\n","env = Environment()\n","print(\"Estados: \", env.state_space)\n","print(\"Acciones: \", env.action_space)\n","print(\"Recompensas: \", env.rewards)"]},{"cell_type":"markdown","source":["Q-Learning"],"metadata":{"id":"B3kDzTkn8EV8"}},{"cell_type":"code","source":["from ast import AsyncFunctionDef\n","import numpy as np\n","\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","alpha = 0.1\n","gamma = 0.9\n","\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  while state != 3:\n","    action = np.random.choice(env.action_space)\n","    next_state = state + action\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","    state = next_state\n","\n","print(\"Función Q-Valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eo9AX-Yp8H6B","executionInfo":{"status":"ok","timestamp":1725888760937,"user_tz":300,"elapsed":229,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"baa4861b-47f7-436c-a69a-53c3db0ff108"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-Valor aprendida: \n","[[ 4.58  6.2 ]\n"," [ 6.2   8.  ]\n"," [ 8.   10.  ]\n"," [ 0.    0.  ]]\n"]}]},{"cell_type":"markdown","source":["Sarsa"],"metadata":{"id":"jBrI8q3I9625"}},{"cell_type":"code","source":["Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  action = np.random.choice(env.action_space)\n","\n","  while state != 3:\n","    next_state = state + action\n","    next_action = np.random.choice(env.action_space)\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","    state = next_state\n","    action = next_action\n","\n","print(\"Función Q-Valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2jY5iMp973a","executionInfo":{"status":"ok","timestamp":1725888665706,"user_tz":300,"elapsed":223,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"ac0fd298-b6be-47a0-e8c6-81bfe4f290c1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-Valor aprendida: \n","[[ 1.46723377  3.4931794 ]\n"," [ 3.72767503  6.64694481]\n"," [ 6.89827986 10.        ]\n"," [ 0.          0.        ]]\n"]}]},{"cell_type":"markdown","source":["Política de Gradiente de Montecarlo"],"metadata":{"id":"DXSYvRA3_LjA"}},{"cell_type":"code","source":["policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n","\n","def average_reward(Q):\n","  return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","  for _ in range(1000):\n","    state = np.random.choice(env.state_space)\n","    while state != 3:\n","      action = np.random.choice(env.action_space, p=policy[state])\n","      next_state = state + action\n","      reward = env.rewards[next_state]\n","      gradient = np.zeros_like(policy[state])\n","      gradient[action] = 1\n","      policy[state] += alpha * gradient * (reward - average_reward(Q))\n","      state = next_state\n","\n","print(\"Política aprendida: \")\n","print(policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I73FFZX5_2T4","executionInfo":{"status":"ok","timestamp":1725889451781,"user_tz":300,"elapsed":236,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"b3c54150-38d2-4004-cc41-79d5fda3f0bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Política aprendida: \n","[[0.5 0.5]\n"," [0.5 0.5]\n"," [0.5 0.5]\n"," [0.5 0.5]]\n"]}]}]}