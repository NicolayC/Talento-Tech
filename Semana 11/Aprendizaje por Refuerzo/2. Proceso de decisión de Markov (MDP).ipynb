{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxeIIs6ybSpSKNio7pptFP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Descripción del MDP"],"metadata":{"id":"wCb7z77EyVLE"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXL7_J5Hx6XF","executionInfo":{"status":"ok","timestamp":1725885632864,"user_tz":300,"elapsed":239,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"0085f21f-fba7-4f36-d6f5-ab612ea68845"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estado actual: B\n","Acción tomada: Arriba\n","Nuevo estado: B\n","Recompensa: 8\n"]}],"source":["import numpy as np\n","\n","estados = ['A', 'B', 'C']\n","acciones = ['Arriba', 'Abajo']\n","recompensas = np.random.randint(0, 10, size=(len(estados), len(acciones)))\n","\n","def transicion_aleatoria():\n","    return np.random.choice(estados)\n","\n","estado_actual = np.random.choice(estados)\n","accion = np.random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","print(f\"Estado actual: {estado_actual}\")\n","print(f\"Acción tomada: {accion}\")\n","print(f\"Nuevo estado: {nuevo_estado}\")\n","print(f\"Recompensa: {recompensa}\")\n"]},{"cell_type":"code","source":["mdp = {\n","    'estados': estados,\n","    'acciones': acciones,\n","    'recompensas': recompensas,\n","    'transiciones': {  # Example: You'll need to define your actual transitions\n","        'A': {'Arriba': {'A': 0.2, 'B': 0.8, 'C': 0.0}, 'Abajo': {'A': 0.5, 'B': 0.0, 'C': 0.5}},\n","        'B': {'Arriba': {'A': 0.3, 'B': 0.6, 'C': 0.1}, 'Abajo': {'A': 0.1, 'B': 0.8, 'C': 0.1}},\n","        'C': {'Arriba': {'A': 0.0, 'B': 0.2, 'C': 0.8}, 'Abajo': {'A': 0.7, 'B': 0.0, 'C': 0.3}},\n","    }\n","}"],"metadata":{"id":"Zb4B1Bbv1bqW","executionInfo":{"status":"ok","timestamp":1725886220292,"user_tz":300,"elapsed":205,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Conceptos de MDP"],"metadata":{"id":"7cSD95nDzPUa"}},{"cell_type":"code","source":["def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n","  valores = {estado: 0 for estado in mdp['estados']}\n","  while True:\n","    delta = 0\n","    for estado in mdp['estados']:\n","      valor_previo = valores[estado]\n","      valores[estado] = sum(mdp['transiciones'][estado][accion][nuevo_estado] * (mdp['recompensas'][mdp['estados'].index(estado), mdp['acciones'].index(accion)] + gamma * valores[nuevo_estado]) for accion in mdp['acciones'] for nuevo_estado in mdp['estados']) # Access numpy array using index of state and action\n","      delta = max(delta, abs(valor_previo - valores[estado]))\n","    if delta < theta:\n","      break\n","  return valores\n","\n","valores_estados = calcular_valor_estado(mdp)\n","print(\"Valores de los estados: \", valores_estados)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gnv9XWUzRSk","executionInfo":{"status":"ok","timestamp":1725886408820,"user_tz":300,"elapsed":203,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"3eec7a10-fe84-4011-92f8-b5bc0fde2a9f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Valores de los estados:  {'A': nan, 'B': nan, 'C': nan}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-2be4d0a9c359>:7: RuntimeWarning: overflow encountered in scalar add\n","  valores[estado] = sum(mdp['transiciones'][estado][accion][nuevo_estado] * (mdp['recompensas'][mdp['estados'].index(estado), mdp['acciones'].index(accion)] + gamma * valores[nuevo_estado]) for accion in mdp['acciones'] for nuevo_estado in mdp['estados']) # Access numpy array using index of state and action\n","<ipython-input-9-2be4d0a9c359>:7: RuntimeWarning: invalid value encountered in scalar multiply\n","  valores[estado] = sum(mdp['transiciones'][estado][accion][nuevo_estado] * (mdp['recompensas'][mdp['estados'].index(estado), mdp['acciones'].index(accion)] + gamma * valores[nuevo_estado]) for accion in mdp['acciones'] for nuevo_estado in mdp['estados']) # Access numpy array using index of state and action\n"]}]},{"cell_type":"markdown","source":["Propiedades de Markov"],"metadata":{"id":"QDHhbw9H2Ssa"}},{"cell_type":"code","source":["def verificar_propiedad_mark(mdp):\n","  for estado in mdp['estados']:\n","    for accion in mdp['acciones']:\n","      if sum(mdp['transiciones'][estado][accion].values()) != 1:\n","        return False\n","  return True"],"metadata":{"id":"0eAGAYUF2m0b","executionInfo":{"status":"ok","timestamp":1725886752538,"user_tz":300,"elapsed":198,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(\"Cumple con la propiedad de Markov\", verificar_propiedad_mark(mdp))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM2UuZK63f6l","executionInfo":{"status":"ok","timestamp":1725886783997,"user_tz":300,"elapsed":196,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"2b487232-9463-4144-89e4-9266d1b9df38"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Cumple con la propiedad de Markov False\n"]}]},{"cell_type":"markdown","source":["Propiedad de recompensa"],"metadata":{"id":"dihn7U6J3n5d"}},{"cell_type":"code","source":["def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  total_acciones = 0\n","  for estado in mdp['estados']:\n","    for accion in mdp['acciones']:\n","      for nuevo_estado in mdp['estados']:\n","        recompensa_total += mdp['transiciones'][estado][accion][nuevo_estado] * mdp['recompensas'][mdp['estados'].index(estado), mdp['acciones'].index(accion)]\n","        total_acciones += 1\n","  return recompensa_total / total_acciones"],"metadata":{"id":"_6Xozmkr3qcl","executionInfo":{"status":"ok","timestamp":1725887089735,"user_tz":300,"elapsed":193,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(\"Recompensa promedio por acción: \", calcular_recompensa_promedio(mdp))"],"metadata":{"id":"yDXBUu384xkz","executionInfo":{"status":"ok","timestamp":1725887105817,"user_tz":300,"elapsed":188,"user":{"displayName":"Nicolay Castellanos","userId":"01339309490713817282"}},"outputId":"7a622a33-4afe-4a9b-d114-53f61a25a160","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Recompensa promedio por acción:  1.9444444444444444\n"]}]}]}